{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/victorenriquenr/Scraping-Propiedades--Santiago-de-Chile/blob/main/ScrapeDepto.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TNusyTtxpDwP"
      },
      "source": [
        "<a href=\"https://chilepropiedades.cl/\"><img src=\"https://miro.medium.com/v2/resize:fit:720/format:webp/0*XMbwmj-4r80bBuIg.jpg\" width=\"400\"> </a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J6Z96OZ7Ulg7"
      },
      "source": [
        "\n",
        "\n",
        "# **Explorando el Mercado Inmobiliario de Santiago de Chile: Extracción de Datos**\n",
        "\n",
        "#### @uthors: Víctor E. Núñez, Julio Torres, Sanber Vizcaya.\n",
        "\n",
        "#### Date: February 29, 2024\n",
        "\n",
        "\n",
        "---\n",
        "---\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7n_fbZc-wKWR"
      },
      "source": [
        "En la era digital actual, la capacidad de obtener información valiosa de forma rápida y eficiente es fundamental para empresas y particulares por igual. En el contexto del mercado inmobiliario de Santiago de Chile, la plataforma https://chilepropiedades.cl emerge como una fuente rica en datos sobre propiedades disponibles para compra o alquiler en la ciudad. Sin embargo, navegar por esta vasta cantidad de información de manera manual puede resultar abrumador y consumir mucho tiempo. Es aquí donde entra en juego el web scraping, una técnica automatizada que permite extraer datos de sitios web de manera rápida y eficiente, simplificando el proceso de análisis de datos.\n",
        "\n",
        "\n",
        "### **EL Código**\n",
        "\n",
        "Para este proyecto,  utilizando Python y bibliotecas como BeautifulSoup y requests, desarrollamos un script que nos permite extraer información detallada de la plataforma de propiedades mencionada. Una versión general del flujo de trabajo del script usado para este proyecto:\n",
        "\n",
        "1. **Configuración de Encabezados y Clases**: Se definen los encabezados de la solicitud HTTP para simular un navegador y se crean las clases necesarias para estructurar los datos extraídos.\n",
        "\n",
        "2. **Funciones de Web Scraping**: Se definen funciones para obtener el contenido HTML de una URL, buscar enlaces internos en la página y extraer datos específicos de cada propiedad inmobiliaria.\n",
        "\n",
        "3. **Iteración a través de Páginas y Propiedades**: Se realiza un bucle para iterar a través de un número determinado de páginas en el sitio web, extrayendo datos de cada propiedad inmobiliaria en cada página.\n",
        "\n",
        "4. **Extracción de Datos y Estructuración**: Para cada propiedad, se extraen datos como dirección, precio, número de habitaciones, etc. Estos datos se estructuran en objetos de clase y se agregan a un DataFrame de Pandas.\n",
        "\n",
        "5. **Almacenamiento y Guardado**: Los datos extraídos se almacenan en un DataFrame de Pandas y finalmente se guardan en un archivo CSV para su posterior análisis.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PUvvI0VTUkz-"
      },
      "source": [
        "## **Librerias**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "mOlIt9pAU4CX"
      },
      "outputs": [],
      "source": [
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "\n",
        "\n",
        "from lxml import html\n",
        "import numpy as np, pandas as pd\n",
        "import re\n",
        "import time"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GFsp-2_5U_zd"
      },
      "source": [
        "## **Scraping**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "BO8dwzpc1A3L"
      },
      "outputs": [],
      "source": [
        "# Establecer el encabezado para simular un navegador web\n",
        "headers = {\n",
        "    \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36\",\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "gx0vTTG71D5G"
      },
      "outputs": [],
      "source": [
        "class Content:\n",
        "  def __init__(self,direccion, precio_CLP, precio_UF, gastos_comunes, habitaciones,\n",
        "               baños, piso, estacionamiento, area_total, area_util,interior,\n",
        "               exterior, servicios, piso_tipo, descripcion, servicios_cercanos,\n",
        "               disponible_para, fecha_publicacion, corredora, ID_publicacion,\n",
        "               año_construccion, Link):\n",
        "    #Inicialización de la clase con los atributos de la propiedad\n",
        "    self.direccion =  direccion\n",
        "    self.Link = Link\n",
        "    self.precio_CLP = precio_CLP\n",
        "    self.precio_UF = precio_UF\n",
        "    self.gastos_comunes =  gastos_comunes\n",
        "    self.habitaciones = habitaciones\n",
        "    self.baños =  baños\n",
        "    self.piso =  piso\n",
        "    self.estacionamiento = estacionamiento\n",
        "    self.area_total = area_total\n",
        "    self.area_util =  area_util\n",
        "    self.interior =  interior\n",
        "    self.exterior = exterior\n",
        "    self.servicios =  servicios\n",
        "    self.piso_tipo =  piso_tipo\n",
        "    self.descripcion =  descripcion\n",
        "    self.servicios_cercanos =  servicios_cercanos\n",
        "    self.disponible_para =  disponible_para\n",
        "    self.fecha_publicacion =  fecha_publicacion\n",
        "    self.corredora =  corredora\n",
        "    self.año_construccion = año_construccion\n",
        "    self.ID_publicacion =  ID_publicacion\n",
        "\n",
        "\n",
        "  def to_dataframe(self):\n",
        "    #Convertir los atributos de la propiedad en un DataFrame de Pandas\n",
        "    data = {\n",
        "        'direccion': self.direccion,\n",
        "        'precio_CLP': self.precio_CLP,\n",
        "        'precio_UF': self.precio_UF,\n",
        "        'gastos_comunes': self.gastos_comunes,\n",
        "        'habitaciones': self.habitaciones,\n",
        "        'baños': self.baños,\n",
        "        'piso': self.piso,\n",
        "        'estacionamiento': self.estacionamiento,\n",
        "        'area_total': self.area_total,\n",
        "        'area_util': self.area_util,\n",
        "        'interior': self.interior,\n",
        "        'exterior': self.exterior,\n",
        "        'servicios': self.servicios,\n",
        "        'piso_tipo': self.piso_tipo,\n",
        "        'descripcion': self.descripcion,\n",
        "        'servicios_cercanos': self.servicios_cercanos,\n",
        "        'disponible_para': self.disponible_para,\n",
        "        'fecha_publicacion': self.fecha_publicacion,\n",
        "        'corredora': self.corredora,\n",
        "        'ID_publicacion': self.ID_publicacion,\n",
        "        'año_construccion': self.año_construccion,\n",
        "        'Link': self.Link\n",
        "        }\n",
        "    df = pd.DataFrame(data)\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "UDNSJhJF3_k5"
      },
      "outputs": [],
      "source": [
        "# Obtener el objeto BeautifulSoup y el parser de HTML de una URL dada\n",
        "def getSoup(url):\n",
        "  req = requests.get(url, headers = headers)\n",
        "  soup = BeautifulSoup(req.text, 'html.parser')\n",
        "  parser = html.fromstring(req.content)\n",
        "  return soup, parser\n",
        "\n",
        "#Extraer enlaces internos de una página web dada\n",
        "def InternalLinks(url):\n",
        "  internalLinks = []\n",
        "  soup, parser = getSoup(url)\n",
        "  links =  soup.find_all('a', href = re.compile(r'/ver-publicacion/venta/.*', re.IGNORECASE))\n",
        "  for link in links:\n",
        "    link = 'https://chilepropiedades.cl'+link.get('href')\n",
        "    if link not in internalLinks:\n",
        "      internalLinks.append(link)\n",
        "  return internalLinks\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j46PBj5sVKfQ",
        "outputId": "28b1a7c4-5341-429c-a7f1-1a3d84d26ee8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extrayendo datos de un total de 1 páginas de departamentos en santiago:\n",
            " \n",
            "Extrayendo datos de un total de 1 páginas de departamentos en las-condes:\n",
            " \n",
            "Extrayendo datos de un total de 1 páginas de departamentos en nunoa:\n",
            " \n",
            "Extrayendo datos de un total de 1 páginas de departamentos en providencia:\n",
            " \n",
            "Extrayendo datos de un total de 1 páginas de departamentos en estacion-central:\n",
            " \n",
            "Extrayendo datos de un total de 1 páginas de departamentos en san-miguel:\n",
            " \n",
            "Extrayendo datos de un total de 1 páginas de departamentos en vitacura:\n",
            " \n",
            "Extrayendo datos de un total de 1 páginas de departamentos en independencia:\n",
            " \n",
            "¡Extracción completada!\n"
          ]
        }
      ],
      "source": [
        "def ScrapePage(url,numPage,delay):\n",
        "    #Ajustando el límite de extracción de páginas.\n",
        "    soup, parser =  getSoup(url)\n",
        "    total_pages = int(soup.find_all('a', class_= \"page-link\")[-1].get('href').split('/')[-1])\n",
        "\n",
        "    if numPage > total_pages:\n",
        "        numPage = total_pages\n",
        "\n",
        "    print('Extrayendo datos de un total de {} páginas de departamentos en {}:'.format(numPage, url.split('/')[-2]) )\n",
        "    print(' ')\n",
        "\n",
        "    direccion = []; Link = [];\n",
        "    precio_CLP = []; precio_UF = [];\n",
        "    gastos_comunes = []; habitaciones = [];\n",
        "    baños = []; piso = [];\n",
        "    estacionamiento = [];\n",
        "    area_total = []; area_util = [];\n",
        "    disponible_para = [];\n",
        "    fecha_publicacion = []; ID_publicacion = [];\n",
        "    interior = []; exterior=[];\n",
        "    piso_tipo = []; servicios = [];\n",
        "    descripcion = []; servicios_cercanos = [];\n",
        "    corredora = []; año_construccion = [];\n",
        "    url_ = url\n",
        "    while numPage != 0:\n",
        "\n",
        "        links =  InternalLinks(url_)\n",
        "        for element in links:\n",
        "            soup,parser = getSoup(element)\n",
        "\n",
        "            #Link------------------------------------------------------\n",
        "            Link.append(element)\n",
        "            #direccion-------------------------------------------------\n",
        "            try:\n",
        "                direccion.append(soup.find('h1').get_text().replace('\\n', '').strip())\n",
        "            except:\n",
        "                direccion.append(np.nan)\n",
        "\n",
        "            #-----------------------------------------------------------\n",
        "            #-----------------------------------------------------------\n",
        "\n",
        "            dict={}\n",
        "            for key,value in zip(soup.find_all('div', class_ = \"clp-description-label col-6\"),\n",
        "                                 soup.find_all('div', class_ = \"clp-description-value col-6\")\n",
        "                                ):\n",
        "                key =  key.get_text().strip()\n",
        "                value =  value.get_text().replace('  ','').strip()\n",
        "                dict[key] =  value\n",
        "\n",
        "            #-----------------------------------------------------------\n",
        "           #Precio\n",
        "            try:\n",
        "                if dict['Valor (CLP aprox.)*:']:\n",
        "                    Valor_CLP_ = dict['Valor (CLP aprox.)*:'].split()[1]\n",
        "                    precio_CLP.append(Valor_CLP_)\n",
        "                    Valor_UF_  = dict['Valor:'].split()[1]\n",
        "                    precio_UF.append(Valor_UF_)\n",
        "            except:\n",
        "                Valor_CLP_  = dict['Valor:'].split()[1]\n",
        "                precio_CLP.append(Valor_CLP_)\n",
        "                Valor_UF_ = dict['Valor (UF aprox.)*:'].split()[1]\n",
        "                precio_UF.append(Valor_UF_)\n",
        "\n",
        "\n",
        "            #Gastos Comunes--------------------------------------------\n",
        "            try:\n",
        "                gastos_comunes.append(dict['Gastos Comunes:'].split()[1])\n",
        "            except:\n",
        "                gastos_comunes.append(np.nan)\n",
        "            #Habitaciones----------------------------------------------\n",
        "            try:\n",
        "                habitaciones.append(int(dict['Habitaciones:']))\n",
        "            except:\n",
        "                habitaciones.append(np.nan)\n",
        "            #Baños-----------------------------------------------------\n",
        "            try:\n",
        "                baños.append(int(dict['Baño:']))\n",
        "            except:\n",
        "                baños.append(np.nan)\n",
        "            #Piso--------------------------------------------------------\n",
        "            try:\n",
        "                piso.append(int(dict['Piso:']))\n",
        "            except:\n",
        "                piso.append(np.nan)\n",
        "            #Estacionamiento------------------------------------------\n",
        "            try:\n",
        "                estacionamiento.append(int(dict['Estacionamientos:']))\n",
        "            except:\n",
        "                estacionamiento.append('No')\n",
        "            #Area Total-------------------------------------------------\n",
        "            try:\n",
        "                area_total.append(dict['Superficie Total:'])\n",
        "            except:\n",
        "                area_total.append(np.nan)\n",
        "            #Area Util-------------------------------------------\n",
        "            try:\n",
        "                area_util.append(dict['Superficie Útil:'])\n",
        "            except:\n",
        "                area_util.append(np.nan)\n",
        "            #Año de construcción:\n",
        "            try:\n",
        "                año_construccion.append(dict['Año Construcción:'])\n",
        "            except:\n",
        "                año_construccion.append(np.nan)\n",
        "\n",
        "\n",
        "            #----------------------------------------------------------\n",
        "            #----------------------------------------------------------\n",
        "\n",
        "            dict_2 = {}\n",
        "\n",
        "            for key,value in zip(soup.find_all('div', class_ = \"col-6 clp-description-label\"),\n",
        "                                 soup.find_all('div', class_ = \"col-6 clp-description-value\")\n",
        "                                ):\n",
        "                key =  key.get_text().strip()\n",
        "                value =  value.get_text().replace('  ','').strip()\n",
        "                dict_2[key] =  value\n",
        "            #----------------------------------------------------------\n",
        "            #Disponible para--------------------------------------------------\n",
        "            try:\n",
        "                disponible_para.append(dict_2['Tipo de propiedad:'])\n",
        "            except:\n",
        "                disponible_para.append(np.nan)\n",
        "            #Fecha de Publicación------------------------------------------\n",
        "            try:\n",
        "                fecha_publicacion.append(dict_2['Fecha Publicación:'])\n",
        "            except:\n",
        "                fecha_publicacion.append(np.nan)\n",
        "            #ID--------------------------------------------------------\n",
        "            try:\n",
        "                ID_publicacion.append(dict_2['Código aviso:'])\n",
        "            except:\n",
        "                ID_publicacion.append(np.nan)\n",
        "            #----------------------------------------------------------\n",
        "            # Servicios\n",
        "            #----------------------------------------------------------\n",
        "            serv_ = {}\n",
        "            if soup.find('ul', class_=\"clp-equipment-list\"):\n",
        "                for element in soup.find('ul', class_=\"clp-equipment-list\").get_text().strip().split('\\n')[:4]:\n",
        "                    try:\n",
        "                        key = element.split(':')[0]\n",
        "                        value = element.split(':')[1]\n",
        "                        serv_[key] = value\n",
        "                    except:\n",
        "                        None\n",
        "\n",
        "            else:\n",
        "                serv_['Interior'] = np.nan\n",
        "                serv_['Exterior'] = np.nan\n",
        "                serv_['Servicios'] = np.nan\n",
        "                serv_['Piso'] = np.nan\n",
        "\n",
        "            #Interior--------------------------------------------------------\n",
        "            try:\n",
        "                interior.append(serv_['Interior'])\n",
        "            except:\n",
        "                interior.append(np.nan)\n",
        "            #Exterior--------------------------------------------------------\n",
        "            try:\n",
        "                exterior.append(serv_['Exterior'])\n",
        "            except:\n",
        "                exterior.append(np.nan)\n",
        "            #Servicios--------------------------------------------------------\n",
        "            try:\n",
        "                servicios.append(serv_['Servicios'])\n",
        "            except:\n",
        "                servicios.append(np.nan)\n",
        "            #Tipo de Piso---------------------------------------------------\n",
        "            try:\n",
        "                piso_tipo.append(serv_['Piso'])\n",
        "            except:\n",
        "                piso_tipo.append(np.nan)\n",
        "\n",
        "            #Descripción-----------------------------------------------------\n",
        "            text =[]\n",
        "            if soup.find('div', class_=\"clp-description-box\"):\n",
        "                try:\n",
        "                    for element in soup.find('div', class_=\"clp-description-box\"):\n",
        "                        text.append(element.get_text().strip())\n",
        "                    descripcion.append(\" \".join(text))\n",
        "                except:\n",
        "                    descripcion.append(np.nan)\n",
        "            else:\n",
        "                descripcion.append(np.nan)\n",
        "            #Servicios Cercanos ---------------------------------------------\n",
        "            if soup.find_all('h2', string = 'Comodidades y Lugares de Interés'):\n",
        "                try:\n",
        "                    list_ = []\n",
        "                    for div in soup.find_all('div', class_= 'amenity-text'):\n",
        "                        Nearby_amenities_ = div.find('p').get_text().replace('', '').split('\\n')\n",
        "                        Nearby_amenities_ = list(filter(lambda x: x.strip(), Nearby_amenities_))\n",
        "                        list_.append(Nearby_amenities_[0].strip()+' ('+Nearby_amenities_[-1].strip()+')')\n",
        "                    servicios_cercanos.append(list_)\n",
        "                except:\n",
        "                    servicios_cercanos.append(np.nan)\n",
        "            else:\n",
        "                servicios_cercanos.append(np.nan)\n",
        "            #Corredora-----------------------------------\n",
        "            if soup.find_all('h2', string='Corredora'):\n",
        "                try:\n",
        "                    Real_estate_agent_ = soup.find('div', class_= \"col-sm-8 clp-user-contact-details-table\").find('a').get_text()\n",
        "                    corredora.append(Real_estate_agent_)\n",
        "                except:\n",
        "                    corredora.append(np.nan)\n",
        "            else:\n",
        "                corredora.append(np.nan)\n",
        "\n",
        "\n",
        "        time.sleep(delay)\n",
        "        numPage = numPage-1\n",
        "        if numPage == 0:\n",
        "            break\n",
        "        next_soup, next_ṕarser = getSoup(url_)\n",
        "        url_ = next_soup.find('link', {'rel':'next'}).get('href')\n",
        "\n",
        "    return Content(direccion, precio_CLP, precio_UF, gastos_comunes, habitaciones,\n",
        "               baños, piso, estacionamiento, area_total, area_util,interior,\n",
        "               exterior, servicios, piso_tipo, descripcion, servicios_cercanos,\n",
        "               disponible_para, fecha_publicacion, corredora, ID_publicacion,\n",
        "               año_construccion, Link)\n",
        "\n",
        "\n",
        "# Creando DataFrames vacíos.\n",
        "dataframe1 = pd.DataFrame()\n",
        "dataframe2 = pd.DataFrame()\n",
        "dataframe3 = pd.DataFrame()\n",
        "dataframe4 = pd.DataFrame()\n",
        "dataframe5 = pd.DataFrame()\n",
        "dataframe6 = pd.DataFrame()\n",
        "dataframe7 = pd.DataFrame()\n",
        "dataframe8 = pd.DataFrame()\n",
        "\n",
        "for element in ['santiago','las-condes', 'nunoa', 'providencia', 'estacion-central',\n",
        "               'san-miguel', 'vitacura', 'independencia']:\n",
        "    start_url = 'https://chilepropiedades.cl/propiedades/venta/departamento/{}/0'.format(element)\n",
        "\n",
        "    #Iniciamos extracción:\n",
        "    content = ScrapePage(start_url,500,1)\n",
        "\n",
        "    new_df = content.to_dataframe()  # Llama al método to_dataframe() del objeto Content.\n",
        "\n",
        "    # Determina qué DataFrame corresponde y concatena el nuevo DataFrame.\n",
        "    if element == 'santiago':\n",
        "        dataframe1 = pd.concat([dataframe1, new_df],axis = 0, ignore_index=True)\n",
        "    elif element == 'las-condes':\n",
        "        dataframe2 = pd.concat([dataframe2, new_df], ignore_index=True)\n",
        "    elif element == 'nunoa':\n",
        "        dataframe3 = pd.concat([dataframe3, new_df], ignore_index=True)\n",
        "    elif element == 'providencia':\n",
        "        dataframe4 = pd.concat([dataframe4, new_df], ignore_index=True)\n",
        "    elif element == 'estacion-central':\n",
        "        dataframe5 = pd.concat([dataframe5, new_df], ignore_index=True)\n",
        "    elif element == 'san-miguel':\n",
        "        dataframe6 = pd.concat([dataframe6, new_df], ignore_index=True)\n",
        "    elif element == 'vitacura':\n",
        "        dataframe7 = pd.concat([dataframe7, new_df], ignore_index=True)\n",
        "    elif element == 'independencia':\n",
        "        dataframe8 = pd.concat([dataframe8, new_df], ignore_index=True)\n",
        "\n",
        "\n",
        "final_df = pd.concat([dataframe1, dataframe2, dataframe3, dataframe4,\n",
        "                     dataframe5, dataframe6, dataframe7, dataframe8], axis = 0).reset_index(drop =  True)\n",
        "#Guardamos\n",
        "final_df.to_csv('./Dataset_DeptoStgoCHL.csv')\n",
        "\n",
        "print('¡Extracción completada!')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyOXzJ3sbNfs4f4rb1cWISz6",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
